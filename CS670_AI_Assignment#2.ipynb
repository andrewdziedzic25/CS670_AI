{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS670 - AI\n",
    "# NJIT at Jersey City\n",
    "# Summer 2023 \n",
    "# Assignment #2 - Gaussian Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e826b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question #1 - MLE of a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82912863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To estimate the parameters of the Gaussian p_model using gradient-based optimization,\n",
    "# we can use the Maximum Likelihood Estimation (MLE) method. MLE is a common approach for \n",
    "# estimating the parameters of a probability distribution based on observed data.\n",
    "# In the case of a Gaussian distribution, we need to estimate the mean (μ) and standard deviation (σ).\n",
    "\n",
    "# Below is the Pyhon implementation that estimates the optimal parameters for the Gaussian p_model\n",
    "# using gradient-based optimization:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2932b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46b8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the negative log-likelihood\n",
    "def neg_log_likelihood(params, data):\n",
    "    mean, std = params\n",
    "    log_likelihood = -np.sum(np.log(np.exp(-(data - mean) ** 2 / (2 * std ** 2)) / (np.sqrt(2 * np.pi) * std)))\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e7ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the gradient of the negative log-likelihood\n",
    "def gradient(params, data):\n",
    "    mean, std = params\n",
    "    d_mean = np.sum((data - mean) / std ** 2)\n",
    "    d_std = np.sum(((data - mean) ** 2 - std ** 2) / std ** 3)\n",
    "    return np.array([-d_mean, -d_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d12795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-based optimization using the L-BFGS-B method\n",
    "def optimize_params(data):\n",
    "    # Initial guess for the parameters\n",
    "    initial_params = [np.mean(data), np.std(data)]\n",
    "\n",
    "    # Minimize the negative log-likelihood using L-BFGS-B\n",
    "    from scipy.optimize import minimize\n",
    "    result = minimize(neg_log_likelihood, initial_params, args=(data,), jac=gradient, method='L-BFGS-B')\n",
    "\n",
    "    # Extract the optimal parameters\n",
    "    optimal_params = result.x\n",
    "\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d68634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "data = np.array([4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f061d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the parameters\n",
    "optimal_params = optimize_params(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828f1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal mean: 6.214285714285714\n",
      "Optimal standard deviation: 2.425418120907092\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the optimal parameters\n",
    "print(\"Optimal mean:\", optimal_params[0])\n",
    "print(\"Optimal standard deviation:\", optimal_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go through the code step by step:\n",
    "\n",
    "# We define the neg_log_likelihood function to calculate the negative log-likelihood of the Gaussian distribution given the data. This function takes the parameters (mean and standard deviation) and the data as input and returns the negative log-likelihood.\n",
    "# We define the gradient function to calculate the gradient of the negative log-likelihood with respect to the parameters. This function takes the parameters and the data as input and returns the gradient.\n",
    "# The optimize_params function performs the gradient-based optimization using the L-BFGS-B method. It takes the data as input and returns the optimal parameters.\n",
    "# We define the given data as a NumPy array.\n",
    "# We call the optimize_params function to estimate the optimal parameters.\n",
    "# Finally, we print the optimal mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code uses the minimize function from the scipy.optimize module, specifically using the L-BFGS-B method,\n",
    "# which is a gradient-based optimization algorithm that supports bounds on the variables. The minimize function \n",
    "# minimizes the negative log-likelihood function by iteratively updating the parameters until convergence. The\n",
    "# negative log-likelihood and its gradient are provided as input to the minimize function \n",
    "# using the args and jac arguments, respectively. The resulting optimal parameters are extracted from the \n",
    "# optimization result and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c353fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The code assumes that you have the necessary dependencies installed, including NumPy and SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b355d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Step 1: The neg_log_likelihood function The neg_log_likelihood function calculates the negative \n",
    "# log-likelihood of the Gaussian distribution given the data. The negative log-likelihood is a measure of \n",
    "# how well the Gaussian distribution with specific parameters fits the observed data. The function \n",
    "# takes two arguments: params (which represents the mean and standard deviation of the Gaussian distribution) and \n",
    "# data (the observed data points). It uses the formula for the Gaussian probability density function (PDF) to calculate\n",
    "# the log-likelihood for each data point and then sums them up. Finally, it returns the negative log-likelihood.\n",
    "\n",
    "# Step 2: The gradient function The gradient function calculates the gradient of the negative log-likelihood with\n",
    "# respect to the parameters (mean and standard deviation). The gradient provides information on the direction \n",
    "# and magnitude of the steepest increase in the negative log-likelihood. The function takes the same arguments\n",
    "# as neg_log_likelihood: params and data. It computes the partial derivatives of the negative log-likelihood function\n",
    "# with respect to the mean and standard deviation using the chain rule. The resulting gradients are\n",
    "# returned as a NumPy array.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb94460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: The optimize_params function The optimize_params function performs the gradient-based optimization \n",
    "# using the L-BFGS-B method. It takes the observed data (data) as input and returns\n",
    "# the optimal parameters (mean and standard deviation) of the Gaussian distribution. Here's how the function works:\n",
    "\n",
    "# First, we initialize the initial guess for the parameters using the mean and standard deviation of the data.\n",
    "\n",
    "# We use the minimize function from the scipy.optimize module to minimize the negative log-likelihood function.\n",
    "# It takes several arguments:\n",
    "\n",
    "# neg_log_likelihood: The objective function to minimize.\n",
    "# initial_params: The initial guess for the parameters.\n",
    "# args=(data,): Additional arguments to be passed to the objective function (in this case, the data).\n",
    "# jac=gradient: The function that calculates the gradient of the objective function.\n",
    "# method='L-BFGS-B': The optimization method to use (L-BFGS-B in this case).\n",
    "\n",
    "# The minimize function performs the optimization by iteratively updating the parameters based on the negative log-likelihood and its gradient until convergence.\n",
    "\n",
    "# Finally, we extract the optimal parameters from the optimization result using result.x and return them.\n",
    "\n",
    "# Step 4: Given data The given data is represented as a NumPy array. In this case, we have an array \n",
    "# data containing the observed data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1524979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Estimating the optimal parameters We call the optimize_params function with \n",
    "# the given data (data) as input. It performs the optimization and returns the\n",
    "# optimal parameters (mean and standard deviation) for the Gaussian distribution. \n",
    "\n",
    "# Step 6: Printing the optimal parameters Finally, we print the optimal mean and standard deviation that \n",
    "# were obtained from the optimization process.\n",
    "\n",
    "# The code leverages the minimize function from the scipy.optimize module, utilizing\n",
    "# the L-BFGS-B optimization algorithm. L-BFGS-B is a gradient-based optimization method that supports bounds \n",
    "# on the variables, making it suitable for estimating the parameters of the Gaussian distribution. The negative \n",
    "# log-likelihood function and its gradient are provided to the minimize function as arguments, and it performs\n",
    "# the optimization to find the optimal parameters that maximize the likelihood of the observed data under \n",
    "# the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1cef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25295d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question #2 - MLE of a Conditional Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# To estimate the parameters of a conditional Gaussian model, we can use Maximum Likelihood Estimation (MLE). MLE aims\n",
    "# to find the parameters that maximize the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d234540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e51ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "x = np.array([8, 16, 22, 33, 50, 51])\n",
    "y = np.array([5, 20, 14, 32, 42, 58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4260809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the conditional Gaussian model: p_model(y|x, w) = N(w*x, sigma^2)\n",
    "# We assume a linear relationship between x and y with parameters w and constant variance sigma^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a14cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the parameters\n",
    "w_initial = 0.0  # Initial value for the parameter w\n",
    "sigma_initial = 1.0  # Initial value for the standard deviation sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e07fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the negative log-likelihood function\n",
    "def negative_log_likelihood(w, sigma):\n",
    "    # Compute the predicted values\n",
    "    y_pred = w * x\n",
    "    \n",
    "    # Compute the negative log-likelihood\n",
    "    nll = -np.sum(np.log(1 / (np.sqrt(2 * np.pi) * sigma)) - 0.5 * ((y - y_pred) / sigma)**2)\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5490623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient of the negative log-likelihood function\n",
    "def gradient_negative_log_likelihood(w, sigma):\n",
    "    # Compute the predicted values\n",
    "    y_pred = w * x\n",
    "    \n",
    "    # Compute the gradients with respect to w and sigma\n",
    "    grad_w = np.sum((y_pred - y) * x / sigma**2)\n",
    "    grad_sigma = np.sum(((y_pred - y) / sigma)**2 - 1)\n",
    "    \n",
    "    return grad_w, -grad_sigma  # Fix the sign of the gradient for sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7bc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform gradient-based optimization using stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f64e501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and number of iterations\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45718b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "w = w_initial\n",
    "sigma = sigma_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc75c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent\n",
    "for iteration in range(num_iterations):\n",
    "    # Compute the gradients\n",
    "    grad_w, grad_sigma = gradient_negative_log_likelihood(w, sigma)\n",
    "    \n",
    "    # Update the parameters\n",
    "    w -= learning_rate * grad_w\n",
    "    sigma -= learning_rate * grad_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac52cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameter w: 0.9696900477450642\n",
      "Estimated parameter sigma: 19.478651962283752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the estimated parameters\n",
    "print(\"Estimated parameter w:\", w)\n",
    "print(\"Estimated parameter sigma:\", sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this code, we initialize the parameters w and sigma with some initial values. Then, we define the \n",
    "# negative log-likelihood function and its gradient. We use stochastic gradient descent (SGD) to perform \n",
    "# gradient-based optimization and update the parameters iteratively. Finally, we print the \n",
    "# estimated values of w and sigma.\n",
    "\n",
    "# Note: This code assumes that the relationship between x and y is linear\n",
    "# with constant variance. If you have a different model in mind, you can modify the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ab3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
