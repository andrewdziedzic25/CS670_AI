{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea28ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To develop the required classifier using the Harvard USPTO Patent Dataset (HUPD), I will use \n",
    "# Hugging Face's Datasets and Transformers libraries. The classifier will be trained on a small subset\n",
    "# of the dataset corresponding to all patent applications submitted in January 2016, with a focus on the \n",
    "# abstract and claims.\n",
    "\n",
    "# Below is the full code, including comments, that loads the dataset, preprocesses the data, trains the classifier, and\n",
    "# performs predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c24285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cache directories for Hugging Face\n",
    "HG_DIR = '/nlp/scr/msuzgun/cache_extra/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = f'{HG_DIR}/transformers'\n",
    "os.environ['HF_HOME'] = HG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HUPD dataset for January 2016 patent applications\n",
    "dataset_dict = load_dataset('HUPD/hupd',\n",
    "    name='sample',\n",
    "    data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "    cache_dir='/u/scr/nlp/data/HUPD',\n",
    "    icpr_label=None,\n",
    "    train_filing_start_date='2016-01-01',\n",
    "    train_filing_end_date='2016-01-31',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbe450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label-to-index mapping for the decision status field\n",
    "decision_to_str = {'REJECTED': 0, 'ACCEPTED': 1, 'PENDING': 2, 'CONT-REJECTED': 3, 'CONT-ACCEPTED': 4, 'CONT-PENDING': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map decision status to string labels\n",
    "def map_decision_to_string(example):\n",
    "    return {'decision': decision_to_str[example['decision']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637206a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label mapping to the training and validation sets\n",
    "train_set = dataset_dict['train'].map(map_decision_to_string)\n",
    "val_set = dataset_dict['validation'].map(map_decision_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the abstracts in the training set\n",
    "train_set = train_set.map(\n",
    "    lambda e: tokenizer((e['abstract']), truncation=True, padding='max_length'),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f080ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the abstracts in the validation set\n",
    "val_set = val_set.map(\n",
    "    lambda e: tokenizer((e['abstract']), truncation=True, padding='max_length'),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for training and validation sets\n",
    "train_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'decision'])\n",
    "val_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and validation sets\n",
    "train_dataloader = DataLoader(train_set, batch_size=16)\n",
    "val_dataloader = DataLoader(val_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the classifier\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier = SVC()\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):  # Number of training epochs\n",
    "    train_loss = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['decision'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    classifier.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['decision'].to(device)\n",
    "\n",
    "            outputs = classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            val_predictions.extend(predicted.tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {train_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the output for a given filing number or unique identifier\n",
    "def predict_patentability_score(filing_number):\n",
    "    # Find the patent application with the given filing number\n",
    "    selected_application = dataset_dict['validation'].filter(\n",
    "        lambda example: example['patent_number'] == filing_number\n",
    "    )\n",
    "\n",
    "    if len(selected_application) == 0:\n",
    "        return \"Patent application not found.\"\n",
    "\n",
    "    # Tokenize the abstract and claims\n",
    "    abstract = selected_application['abstract'][0]\n",
    "    claims = selected_application['claims'][0]\n",
    "    tokenized_abstract = tokenizer(abstract, truncation=True, padding='max_length')\n",
    "    tokenized_claims = tokenizer(claims, truncation=True, padding='max_length')\n",
    "\n",
    "    # Convert tokenized data to tensors\n",
    "    input_ids_abstract = torch.tensor(tokenized_abstract['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask_abstract = torch.tensor(tokenized_abstract['attention_mask']).unsqueeze(0).to(device)\n",
    "    input_ids_claims = torch.tensor(tokenized_claims['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask_claims = torch.tensor(tokenized_claims['attention_mask']).unsqueeze(0).to(device)\n",
    "\n",
    "    # Pass the tokenized data through the classifier\n",
    "    abstract_patentability_score = classifier(input_ids=input_ids_abstract, attention_mask=attention_mask_abstract)\n",
    "    claims_patentability_score = classifier(input_ids=input_ids_claims, attention_mask=attention_mask_claims)\n",
    "\n",
    "    return abstract, claims, abstract_patentability_score.item(), claims_patentability_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: predict patentability score for a filing number\n",
    "filing_number = 'US20160000123'\n",
    "abstract, claims, abstract_score, claims_score = predict_patentability_score(filing_number)\n",
    "print(f\"Abstract:\\n{abstract}\")\n",
    "print(f\"Claims:\\n{claims}\")\n",
    "print(f\"Patentability Score (Abstract): {abstract_score}\")\n",
    "print(f\"Patentability Score (Claims): {claims_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of the Classifier:\n",
    "\n",
    "# Step1 : Loading and Preprocessing:\n",
    "# - The code loads the HUPD dataset for January 2016 patent applications using the `load_dataset` function from the `datasets` library.\n",
    "# - The dataset is filtered based on the specified filing date range, and label-to-index mapping is defined for the decision status field.\n",
    "# - The tokenizer (`AutoTokenizer`) is initialized to tokenize the abstracts of the patent applications.\n",
    "\n",
    "# Step 2: Tokenization and Data Preparation:\n",
    "# - The abstracts in the training and validation sets are tokenized using the tokenizer and preprocessed with truncation and padding.\n",
    "# - The datasets are set in the torch format with the required columns: `input_ids`, `attention_mask`, and `decision`.\n",
    "# - Data loaders (`DataLoader`) are created for the training and validation sets, enabling batch processing during training.\n",
    "\n",
    "# Step 3: Classifier Training:\n",
    "# - The classifier, an SVM model (`SVC`),\n",
    "# is initialized.\n",
    "# - The training loop runs for a specified number of epochs.\n",
    "# - For each epoch, the classifier is trained using batches of input data.\n",
    "# - The optimizer and criterion (not shown in the code) can be defined according to the specific requirements.\n",
    "# - After each epoch, the classifier is evaluated on the validation set to calculate the validation accuracy.\n",
    "\n",
    "# Step 4: Predicting Patentability Score:\n",
    "# - The `predict_patentability_score` function takes a filing number as input.\n",
    "# - The function retrieves the corresponding patent application from the validation set.\n",
    "# - The abstract and claims are tokenized and converted to tensors.\n",
    "# - The tokenized data is passed through the trained classifier to obtain the predicted patentability scores for the abstract and claims.\n",
    "# - The function returns the abstract, claims, and the patentability scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
